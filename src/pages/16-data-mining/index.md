---
title: '数据挖掘知识点总结'
date: '2021-01-22'
cta: '机器学习'
---

# 数据挖掘

## 第一章：绪论

1. 数据挖掘是什么？什么是知识发现？（KDD）

*数据挖掘是一个发现知识的过程，也就是提取知识，并且该过程是由程序来自动实现。*

*知识发现是描述提取知识整个过程的。*

> 数据挖掘只是其中的一环，对于数据挖掘之前的工作而言是数据预处理，之后的工作是数据可视化。

2. 机器学习和数据挖掘的区别？

*机器学习重过程而数据挖掘重结果，或者说前者偏理论后者偏实践。*

> 二者本质上是没有区别的，都是从数据中获取信息。或者说是从不同的角度来看待同一个东西，只不过侧重不同。数据挖掘倾向于半自动化，而机器学习更倾向于自动化。据说取这么多的名字是为了拉取更多的研究经费。

3. 分类和回归的解释以及区别（P5）

*输出变量离散是分类，而输出变量连续则是回归。*

> 其实很好理解，例如属性是性别，那么分为男女两类。这就是离散型数据，因为只有两类，数据是分散的。再例如好瓜，坏瓜和一般瓜，也是离散型的数据。针对需要建立分类模型，对数据类别进行划分。 
> 对于连续型数据而言，以房价为例，每平米的价格会在不同数值上波动，不可能只是整数或者有限的几种价格。数据是连续的，建立回归模型。

以下内容摘自：李航《统计学习方法》p4 

* 输入变量与输出变量均为连续变量的预测问题是回归问题；
* 输出变量为有限个离散变量的预测问题成为分类问题；
* 输入变量与输出变量均为变量序列的预测问题成为标注问题。

:::tip

但是！！！以上内容是为了方便理解而写，也是我最初的认知。

但是其实不是这样的，不能说不对，只能说不准确。也就是说以上内容是具体化了，下面内容则是描述其本质，不失一般性。

以下内容是从数学严谨性的角度而言，或者说从更深层的角度来思考分类和回归问题。不理解也无所谓，其实很好理解。

分类和回归是从输出空间是否**可度量**来区分的！

什么是可度量的空间？

依旧以好瓜，坏瓜为例，好瓜，坏瓜的组成了输出空间，但是好瓜，坏瓜之间好坏的程度是无法度量的，当模型分类错误之时我们得到得信息只能是分类错误值累加。但是房价数据就不同了。假如现实情况是 6w 每平米，而模型计算出来得数据是 7w 每平米。我们可以清楚的计算预测值和实际值之间的差距，可以采用 MSE 等特征值来进行更多全面的分析。这是一个可度量的空间，也可以理解为从损失函数的角度来分析。

综上，输出空间无法度量是分类，反之则是回归。

> 参考：https://www.zhihu.com/question/21329754/answer/204957456


## 第二章：数据

1. 属性的四种类型 （P17）

属性数据分为标称，序数，区间和比率四种类型。

以下为个人理解：（这一块存疑，目前存在部分疑惑）

* 数据分为离散型数据和连续型数据，对于前者而言是从定性的角度来区分，对于后者而言是从定量的角度来区分。
* 对于离散型数据而言，如果数据之间不存在相对顺序，仅仅以名字来区分将其称之为标称型。例如性别，男女没有顺序。
* 但是如果离散型数据存在相对顺序，那么称之为序数型。例如好瓜，一般瓜，坏瓜。
* 对于连续型数据而言，如果数据可以相互加减并且是有意义的，那么长称之为区间型。例如日期，温度。
* 对于连续型数据，如果值之间可以乘除，那么称之为比率型。例如长度。

## 第三章：探索数据

1. 频数和众数，其中众数是频数的最大值。
2. 百分位数，对于有序数据更有意义。
3. 均值和中位数。
4. 极差和方差。

## 第四章：决策树

1. 信息熵的概念

*信息熵用于描述信息的混乱程度。*

除此之外还有一个热力熵的概念，热力熵的准确定义是用于衡量*分子的混乱程度。* 这个初中物理学过。其实可以将热力熵理解为信息熵的特例，二者本质是相同的,都是在表示 **xxx 的混乱程度。**

> 信息是不确定性的度量！这句话出自《香农传》，我第一次看到的时候无法理解。首先信息存在不确定性，但是如何衡量信息的不确定？这个不确定性多大多小？其实就是一个范围问题，而熵是所有可能性所组成的空间。信息的不确定性程度越大意味着信息越混乱从而导致熵很大。例如信息是水，熵就是水杯，但是水的体积要小于等于水杯。
参考：[信息为什么还有单位，熵为什么用 log 来计算？](https://mp.weixin.qq.com/s?__biz=MzIxNDI5MDk0MQ==&mid=2247484035&idx=1&sn=0d2542daefb495ce93fdb4bde0d9c349&chksm=97a89a11a0df13073fd74513e18dfcd1f0d979437ad73d5c044a7467f879ea7b27a63993ea8f&cur_album_id=1342746447506374657&scene=189#rd)


2. 如何计算信息熵

信息熵公式分为离散均匀分布（$log_2{m}$）和离散分布（$-\sum p_ilog_2{p_i}$）两种类型。其实前者就是后者的特例，记忆后者即可。

> 以选择题为例，目前存在四个选项，其他信息一概不知，下面是信息熵的计算过程。这是离散均匀分布的，也就是每个选项的可能性都是 $1/4$ 。所以计算步骤为：$log_2{1/p} = log_2{1/4}  = 2$

> 离散分布型：假如第三个选项的概率是 50% ，那么分布就变了，并且每个选项的选择的概率不同，也就不均匀了。
$1/6 * log_2{6/1} + 1/6 * log_2{6/1} + 1/2 * log_2{2/1} + 1/6 * log_2{1/6} = 1.79$

3. 信息增益

* ID3 算法是以信息增益来选择特征的。
* 计算公式：$g(D,A) = H(D) - H(D|A)$ 。其中 $H(D)$ 是经验熵，而 $H(D|A)$ 则是条件熵。
* 信息增益的计算步骤：

> 以如下数据集为例，这个案例来源于《统计学习方法》第五章。

ID | 年龄 | 有工作 | 有自己的房子 | 信贷情况 | 类别
:-: | :-: | :-: | :-: | :-: | :-: 
1 | 青年 | 否 | 否 | 一般 | 否
2 | 青年 | 否 | 否 | 好 | 否
3 | 青年 | 是 | 否 | 好 | 是
4 | 青年 | 是 | 是 | 一般 | 是
5 | 青年 | 否 | 否 | 一般 | 否
6 | 中年 | 否 | 否 | 一般 | 否
7 | 中年 | 否 | 否 | 好 | 否
8 | 中年 | 是 | 是 | 好 | 是
9 | 中年 | 否 | 是 | 非常好 | 是
10 | 中年 | 否 | 是 | 非常好 | 是
11 | 老年 | 否 | 是 | 非常好 | 是
12 | 老年 | 否 | 是 | 好 | 是
13 | 老年 | 是 | 否 | 好 | 是
14 | 老年 | 是 | 否 | 非常好 | 是
15 | 老年 | 否 | 否 | 一般 | 否

首先计算经验熵，最后一列是要估计的特征，是和否两种类别分别占比 $9/15$ 和 $6/15$ 。

根据香农公式（$-\sum p_ilog_2{p_i}$），经验熵的计算过程为 

$H(D) = -\frac{9}{15}log_2{\frac{9}{15}} - \frac{6}{15}log_2{\frac{6}{15}} = 0.971$

计算信息增益，其中 A1，A2，A3，A4 分别表示年龄，工作，房子，信贷四种特征。

以 A1（年龄）为例，青年，中年，老年的占比均为 $\frac{5}{15}$ 。

$g(D,A1) = H(D) - [\frac{5}{15}H(D_1) + \frac{5}{15}H(D_2) + \frac{5}{15}H(D_3)] = 0.971 - [\frac{5}{15}(-\frac{2}{5}log_2\frac{2}{5} -\frac{3}{5}log_2\frac{3}{5}) + \frac{5}{15} (-\frac{3}{5}log_2\frac{3}{5} -\frac{2}{5}log_2\frac{2}{5}) + \frac{5}{15}(-\frac{4}{5}log_2\frac{4}{5} -\frac{1}{5}log_2\frac{1}{5})]$

然后再分别计算 A2，A3，A4 的信息增益，计算步骤同上。取最大值作为最优特征即可！

4. 信息增益比

信息增益随着数据集经验熵的改变而改变，该值是相对的，么有绝地意义。为了解决这个问题，可以在信息增益的基础上除经验熵，将结果称之为信息增益比（$g_R(D,A) = \frac{g(D,A)}{H(D)}$）。C4.5 算法是据此选择最优特征的。


> 参考：[第5章决策树-习题](https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter5/chapter5)

5. ID3 建树过程

ID3 只有树的生成部分，容易导致过拟合。

6. 剪枝过程

为什么要剪枝？

*因为过拟合。*

> 如果决策树太复杂或者太庞大会导致过拟合，因为泛化能力不行。这里的复杂表示吸收了过多的训练集的特征。

如何剪枝？

*通过最小化损失函数实现剪枝目的。*

> 针对叶子节点进行剪枝，剪枝前后树结构肯定发生了改变，如何评估改变程度大小？目前而言则是构造一个损失函数，损失函数用于判断训练数据和模型之间的拟合程度。通过损失函数来计算剪枝前后模型拟合程度的变化从而判断是否要进行剪枝。如果减少就说明剪枝是有效的（因为剪枝后训练数据和模型之间的拟合程度增加了），反之不能剪去此处。（损失函数我还没看明白 ��）

7. 先剪枝和后剪枝

先剪枝指在决策树生成过程中就伴随着剪枝，这种剪枝策略避免了产生复杂子树。

> 例如信息增益小于某个阈值，那么就不对此进行扩展。但是缺点也很明显！例如阈值如何设定？阈值低了会导致过拟合，高了则会导致欠拟合。

后剪枝指决策树的生成过程和剪枝过程是分离的。

> 也就是剪枝过程中尽最大可能生长，能长多复杂就长多复杂。后剪枝是按照自底向上的过程针对叶子节点进行修剪，可以采用新的叶节点替换子树，也可以采用最常用的分支来代替子树。后剪枝的效果较好，因为有些“大有可为”的子树可能在先剪枝中“夭折”。

8. Hunt 算法过程

p93 


9. 基尼指数计算

p97

## 第五章：分类

1. KNN

一种懒惰式的分类方法，不需要建立模型，但是依赖 K 值选取。

2. 贝叶斯分类器。

贝叶斯其实就是分子比分母。全概率公式是分母，分子则是条件概率。

3. 神经网络

4. 核函数

核函数实现了低维到高维的转换，从而达到分类的目的。

> 例如对于人而言，从对象的角度考虑都是一维的，但是从性别的角度考虑可以分为男女。此处核函数可以视作从对象到性别的变换。（帮助理解而已，可能不准确）

5. 组合方法

组合方法由多个分类器组成，

6. Bagging 和 Boosting

Bagging 是有放回的抽取，而 Boosting 则是每次迭代后重点关注分类错误的对象。前者可以并行处理，而后者依赖前一阶段的结果所以只能串行处理。

参考：[Bagging和Boosting的区别（面试准备）](https://www.cnblogs.com/earendil/p/8872001.html)


7. 混淆矩阵 ROC AUC 曲线

参考：https://www.bilibili.com/video/BV1wz4y197LU

## 第六章：关联分析

1. 项集和频繁项集

项集其实就是属性，k 个项集其实就是 k 个属性。

> 例如牛奶是一个项集，{啤酒，尿布，牛奶} 是一个 3- 项集，其实就是属性。

频繁项集是项集出现的频次不能低于阈值。

2. 支持度

支持度其实就是项集所占百分比，给定条件在数据集中的占比。

3. 规则

规则指频繁项集中所存在的关系。

4. Apriori

会算即可：https://www.bilibili.com/video/BV1AJ411x7sf

5. FP 增长算法

无候选项集，同 Apriori 算法相比不需要频繁扫描，节省空间效率和时间效率。

## 第八章：聚类

1. Kmeans 聚类

算法流程：1. 设置 K 值。 2. 初始化簇心。 3. 聚类 4. 重新计算簇心 5. 回到步骤 3 。

特点：无监督学习。对噪点敏感！K值选取问题。

2. 基本概念

什么是簇？

簇的定义是对象组。

> 其实就是分类，一类就是一簇。

什么是聚类？

聚类就是发现数据对象之间的关系，将数据对象进行分组。

3. DBSCAN

算法流程：

   1. 划分核心点，边界点，噪点。
   2. 剔除噪点。
   3. 连接核心点距离小于阈值点。
   4. 核心点成为簇。
   5. 指派边界点到簇中。

特点：抗噪声，可以发现 Kmeans 不曾发现的簇。数据维度高时计算开销大，簇密度大时 DBSCAN 难以处理。

## 第十章：异常检测

有一道选择（略）

## 参考 📕 

1. 《数据挖掘导论》主要参考书，因为这本书是教材。
2. 《统计学习方法》李航。
3. 《机器学习》周志华。

> 写于 2021/01/23 11:49